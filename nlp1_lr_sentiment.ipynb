{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2c1a90-f8d8-4453-916d-b9985e00398a",
   "metadata": {},
   "source": [
    "# Supervised ML & Sentiment Analysis\n",
    "## Destekli Öğrenme\n",
    "Destekli makine öğrenmesinde elimizde bir input vardır. Bu X değeri modele girer ve Y_hat(tahmin) değerini hesaplar. Gerçek (Y) değeri ve tahin edilen (Y_hat) değerini karşılaştırmak için bir Cost fonksiyonu kullanırız. Bu cost fonksiyonu bize tahmin edilenin ne kadar yanlış olduğunu döndürür. Bu fonksiyona göre parametreleri(weight and bias) güncelleriz. Bu sayede daha doğru Y_Hat değerleri üretiriz. \n",
    "## Sentiment Analysis\n",
    "Metinlerde sentiment analizi yapmak için benzer bir süreç kullanırız. Burada metin alınır ve bu metin özellik vektörlerine çevirilir. Bu vektör modele gönderilir ve metinin positif mi ne negatif mi olduğu yönünde bir çıktı üretir. Elimizdeki gerçek etikete göre logistic regression modelimizi eğititiriz ve bu modeli metinlerin duygu durumlarını tespit etmek için kullanırız. \n",
    "\n",
    "### Özellik Vektörü Oluşturma\n",
    "Metinler: \n",
    "[metin_1,metin_2,...metin_m] -> I love this movie, ..., ..., I hated the movie  \n",
    "\n",
    "V(Vocabulary) = [I,love, this, movie, ..., hated, the]  \n",
    "#### Özellik Çıkarma \n",
    "Aşağıdaki yorum için özellik vektörü çıkaralım. Burada elimizdeki yorumda vocabularyde olan kelimeler için 1 olmayanlar için 0 koyarak özellik vektörü çıkartabiliriz.   \n",
    "I love this movie  \n",
    "[I love this movie, ..., I hated the movie]   \n",
    "[1, 1, 1, 1, ..., 0, 0, 0, 0]  \n",
    "Bu gösterime sparse gösterim denir. Bu yöntem çok sağlıklı değildir. Burada vektörün boyu vocabularyde bulunan kelime sayısı kadar olacaktır. Her yorum için çok fazla 0 değer olacaktır.  \n",
    "[x0,x1,x2,...,xn]  -> Eğitim ve tahmin uzun sürecek  \n",
    "n = V  \n",
    "\n",
    "### Pozitif ve Negatif Sayıları \n",
    "Corpus: Yazılmış metinler topluluğuna denir. Örneğin IMDB film yorumları bizim corpusumuz olabilir. Yada bir yazarın tüm kitapları bizim corpusumuz olabilir.  \n",
    "\n",
    "Corpus->  \n",
    "I love this movie [P]  \n",
    "I like the movie [P]  \n",
    "I bored the movie [N]    \n",
    "I hated the movie [N]    \n",
    "Yukarıda metinde pozitif ve negatif metinlerde geçen kelimlerin kaç kere geçtiğini yazarak pos freq ve neg freq oluşturabiliriz. \n",
    "Bundan yararlanarak örneğin. \"I love this movie\" için pozitif ve negatif değeri hesaplayabiliriz. Buradan elde ettiğimiz değerler bizim feature vektörümüz olur.   \n",
    "I love this movie -> Xm = [1,sum(freqs(w,1),sum(freqs(w,0)]    \n",
    "sum(freqs(w,1)) -> 4+1+1+4 : 10  \n",
    "sum(freqs(w,0)) -> 4+0+0+4 : 8  \n",
    "Bu metin artık Xm = [1,10,8] 3 boyutlu vektörü ile gösterilebilir. 1 bias, 10 positif özellikler, 8 negatif özellikler.   \n",
    "\n",
    "### Preprocessing\n",
    "Metinde buluanan model için pek fazla anlam ifade etmeyecek kelimelerin, noktalama işaretlerinin düzenlenerek kaldırılması gerekir. \n",
    "* Stopwords ve punctuations  \n",
    "Stopwordler neredeyse tüm metinlerde geçen ve metni ayırma noktasında herhangi bir anlam ifade etmeyen kelimlerdir. Bunlar ingilizce için and, is, are, at, has, for, a gibi kelimelerdir.  \n",
    "Puctuationlarda(Noktalama işaretleri) metinden özellik çıkarmada işimize yaramayacaklardır. [, . : ! \" ']  \n",
    "* URLs  \n",
    "Metinde buluanan URL ler sentiment analizinde herhangi bir anlam ifade etmeyecektir. \n",
    "* Stemming ve lowercasing  \n",
    "Herhangi bir kelimenin eklerini kaldırıp sadece kökünü almak için kullanılır.  \n",
    "Örneğin: going -> go (-ing ekini kaldırdık) yada  \n",
    "loving , loved, love kelimeleri için kelimenin kökü \"lov\" olacaktır. Burada lov kelimesi anlamlı değildir ancak stemming ile kelimenin kökü bulunurken anlam aranmamaktadır.\n",
    "Aynı zamanda tüm kelimeler küçük harfe çevirilir. Love, LOVE, love -> love olacaktır.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6e5afb4-f72c-4cc6-b0fd-6baa99862082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Python NLP kütüphanesi\n",
    "from nltk.corpus import twitter_samples # NLTK kütüphanesinde bulunan twitter veriseti\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83952cfb-0eff-4f7b-b29c-347683b69e05",
   "metadata": {},
   "source": [
    "Aşağıdaki verisetinde positif ve negatif tweetler bulunuyor. Veri setinde 5000 posizif 5000 negatif tweet bulunmaktadır. Elimizdeki veriler dengeli bir şekilde dağılmış olması model oluşturmada işimize yarayacaktır. Ancak gerçek dünyada dağılım bu şekilde olmayabilir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f39b68-15f5-4ad4-8e07-29cf8a09417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\bagat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twitter verisetini indirmek için\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e3a57-4f69-45da-a266-3a1339a666f1",
   "metadata": {},
   "source": [
    "Metinleri indirmek için string() metodunu kullanabiliriz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cfd6ba8-df16-4d64-a8c2-8265ea09a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweet = twitter_samples.strings(\"positive_tweets.json\")\n",
    "all_negative_tweet = twitter_samples.strings(\"negative_tweets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756eecb-719b-41d3-95b7-e86cf2af8f68",
   "metadata": {},
   "source": [
    "Pozitif ve negatif tweetler örnek sayısı ve içerikleri hakkında bilgi edinmek için aşağıdaki gibi bilgileri yazdıralım. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9752df3-26f6-4923-ac10-c1a5956a61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pozitif tweet sayısı: 5000\n",
      "Negatif tweet sayısı: 5000\n",
      "\n",
      "Pozitif tweet verisinin tipi: <class 'list'>\n",
      "Her bir tweet verisinin tipi: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pozitif tweet sayısı: {len(all_positive_tweet)}\")\n",
    "print(f\"Negatif tweet sayısı: {len(all_negative_tweet)}\")\n",
    "\n",
    "print(f\"\\nPozitif tweet verisinin tipi: {type(all_positive_tweet)}\")\n",
    "print(f\"Her bir tweet verisinin tipi: {type(all_positive_tweet[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406614c5-999a-4434-aa72-ae7b8e39fdc0",
   "metadata": {},
   "source": [
    "Veriler liste halinde tutuluyor. Her bir tweet string olarak tutuluyor. Matplotlib ile veriyi görselleştirebiliriz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2df0caf-ee7b-40ca-ae05-7f7e87f508f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "labels = [\"Pozitif\", \"Negatif\"]\n",
    "sizes = [len(all_positive_tweet),len(all_negative_tweet)]\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\",shadow=True,startangle=90)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f280525-9463-43f4-a36b-d4e894cc9b76",
   "metadata": {},
   "source": [
    "### Raw text'e bakalım\n",
    "Herhangi bir işlem yapmadan önce elimizdeki verilere bakıp hangi işlemleri yapacağımızı planlamamız gerekir.  \n",
    "Aşağıdaki random olarak pos. ve neg. tweetleri yazdıralım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7780246a-a217-4d37-948a-968f5ba1d561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos : @FindBenNeedham it's my birthday today so for my birthday wish I hope there's good news about Ben soon :-)\n",
      "Neg : sehun seems so skinny these days :(((\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pos : {all_positive_tweet[random.randint(0,5000)]}\")\n",
    "print(f\"Neg : {all_negative_tweet[random.randint(0,5000)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22418387-e7e8-45fc-860a-195d0d7cb4be",
   "metadata": {},
   "source": [
    "### Preprocess adımına geçelim \n",
    "Data processing işlemi ML projelerinde en önemli adımdır. Verinin temizlenmesi, modele uygun formata getirilmesi adımları oldukça kritiktir. NLP'de temel preprocessing adımları aşağıdaki gibidir.   \n",
    "* Tokenizing the string (Tokenlere çevirme)\n",
    "* Lowercasing (tüm kelimeleri küçük harflere çevirme)\n",
    "* Removing stopwords and punctuations (Stopwordslerin ve noktalama işaretlerinin kaldırılması)\n",
    "* Stemming (Kelimenin kökünün bulunması)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1b543c-4c50-452b-9074-9262281d8e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
     ]
    }
   ],
   "source": [
    "tweet = all_positive_tweet[2277]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223028bf-1432-4f42-802d-c755e83accfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bagat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\") #stopwordsleri indirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4463f4f4-bf57-4549-bef9-d8a697d96747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expression işlemleri için\n",
    "import string # string işlemleri için\n",
    "\n",
    "from nltk.corpus import stopwords #stopwords verileri\n",
    "from nltk.stem import PorterStemmer # stemming için\n",
    "from nltk.tokenize import TweetTokenizer # stringi tokenize etmek için"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46870759-fa9e-4f1e-b3c9-a877dbd24be0",
   "metadata": {},
   "source": [
    "## Hyperlink ve Twitter İşaretlerinin Kaldırılması \n",
    "Twitterda çok fazla kullanılan hashtag, retweet işareti ve hyperlinkleri kaldırabiliriz. Bunun için \"re\" kütühanesini kullanacağız. Eşleşmeleri sub() metodu ile kaldıracağız. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a678f64-3b53-4f9b-942d-4c17ed503f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Preprocessing\n",
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "\n",
      "After Preprocessing\n",
      "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
     ]
    }
   ],
   "source": [
    "print(\"Before Preprocessing\\n\"+tweet)\n",
    "\n",
    "# eskiden kullanılan \"RT\" tagını kaldıralım \n",
    "tweet2 = re.sub(r\"^RT[\\s]+\",'',tweet)\n",
    "\n",
    "# hyperlinkleri kaldıralım \n",
    "tweet2 = re.sub(\"https?://[^\\s\\n\\r]+\",\"\",tweet2)\n",
    "\n",
    "# hashtagleri kaldıralım \n",
    "tweet2 = re.sub(r\"#\",\"\",tweet2)\n",
    "\n",
    "print(\"\\nAfter Preprocessing\\n\"+tweet2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e724f09-93e8-44fa-a3ec-37b3b6ac0abb",
   "metadata": {},
   "source": [
    "## Tokenize the String\n",
    "Tokenize işlemi metnin kelimelere ayrılması işlemidir. Bu adımda hem kelimleri ayırıp hemde bunları küçük harflere çevireceğiz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0900a806-fc33-43ce-acf9-7ec5d13c1eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization\n",
      "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n",
      "\n",
      "After Tokenization\n",
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Tokenization\\n\"+tweet2)\n",
    "\n",
    "# tokenizer classında obje oluşturalım\n",
    "tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True) \n",
    "\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print(\"\\nAfter Tokenization\")\n",
    "print(tweet_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1ad52-3da7-456b-a8e5-461f0830f55c",
   "metadata": {},
   "source": [
    "## Stopwordslerin ve noktalama işaretlerinin kaldırılması \n",
    "Bu adımda stopwordsleri ve noktalama işaretlerini nltk kütüphanesi kullanarak kaldıracağız. İngilizce için kullanılan stop wordsleri almak için \"english\" değerini göndereceğiz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "917bbef5-8125-4579-a9e7-3c264b43e678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words \n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Pucntiuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = stopwords.words(\"english\")\n",
    "\n",
    "print(\"Stop words \\n\")\n",
    "print(stopwords_english)\n",
    "\n",
    "print(\"\\nPucntiuation\\n\")\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fba6c-3e90-4400-a459-f265e9292f3e",
   "metadata": {},
   "source": [
    "Yukarıda gördüğümüz stopwordsler bazı durumlarda işimize yarayabilir. Bu durumlarda stopwords listesini düzenlememiz gerekebilir ama bu örnekte hepsini kullanacağız.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8131be3-4a15-4765-a63f-be600a75fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n",
      "Stopwordsler ve noktalama işaretleri kaldırılmış: \n",
      "\n",
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens)\n",
    "\n",
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens:\n",
    "    if (word not in stopwords_english and word not in string.punctuation):\n",
    "        tweets_clean.append(word)\n",
    "print(\"Stopwordsler ve noktalama işaretleri kaldırılmış: \\n\")\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c950271-58ab-4383-ae59-85f0be5b1ed0",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "Stemming kelimenin kökünün bulunması için kullanılır. Bu vocabulary boyutunu azaltmamıza yarar.  \n",
    "Bazı durumlarda stemming işlemi kelimenin kökünü tam olarak bulamabilir. Buna örnek olarak\n",
    "* happy \n",
    "* happiness \n",
    "* happier  \n",
    "verilebilir. Burada kök happ olacaktır. Bu da yanlış sonuçlar verebilir. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b341bef-3bf2-4b78-ad62-0476ac2902da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n",
      "stemmed words: \n",
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print(tweets_clean)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tweets_stem = []\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)\n",
    "    tweets_stem.append(stem_word)\n",
    "    \n",
    "print(\"stemmed words: \")\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d10bd-b479-4bbd-a2da-23d8ac783b7f",
   "metadata": {},
   "source": [
    "Bir tweet için preprocess kısmını tamamladık. Bunu tüm tweetler için yapamamız gerekiyor. Bunun için bir yukarıda yaptığımız işlemler ile bir fonksiyon yazalım. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6518dfb0-a4e8-4be9-88e3-d3526ee708af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ecaa4-75c2-42a8-941d-6b6c862c1c16",
   "metadata": {},
   "source": [
    "### Genel Bakış \n",
    "Text -> Preprocess -> Feature Extraction   \n",
    "Text -> Preprocess -> [1,10,8] (1 bias, 10 positif değer, 8 negatif için aldığı değer)  \n",
    "[1 X1 X2] -> [1,10,8]   \n",
    "Yukarıda gibi tüm kelimeler için bu işlemi yapıp (m,3) bir matrix elde edebiliriz.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b23869-637f-41c4-8d25-a300e0153e5e",
   "metadata": {},
   "source": [
    "Sentiment analizinde kullanmak için bir kelimenin pozitif ve negatif etiketli cümlelerde kaç kere geçtiğini bulabiliriz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7396827f-5f1a-403d-bf67-e93030489386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam tweet sayısı: 10000\n"
     ]
    }
   ],
   "source": [
    "# tüm tweetleri birleştirelim \n",
    "tweets = all_positive_tweet + all_negative_tweet\n",
    "\n",
    "print(\"Toplam tweet sayısı: \"+ str(len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da6121-ae76-475e-930a-0045291c50eb",
   "metadata": {},
   "source": [
    "Tweetlerin sentimenlerine göre label adında bir array oluşturalım. Bu arrayde ilk 5000 veri 1 olarak son 5000 veri ise 0 olarak etiketlenecek. \n",
    "* np.ones() - 1 lerden array oluştur\n",
    "* np.zeros() - 0 lardan array oluştur\n",
    "* np.append() - 2 arrayi birleştir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d2e3dae-e146-43a2-9505-60c314ac8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.ones(len(all_positive_tweet)),np.zeros(len(all_negative_tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025a00d-8ddb-40e5-8f7f-2116d62b8c8e",
   "metadata": {},
   "source": [
    "## Kelime frekans sözlüğü (word freq dict)\n",
    "Bunun için build_freq adında bir fonksiyon oluşturalım.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0486a36f-3259-4fa7-8623-a5505cf759a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets,ys): \n",
    "    # tweets: list of tweet \n",
    "    # ys: mx1 label arrayi \n",
    "    \n",
    "    # numpy arrayini liste çevirelim \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    \n",
    "    # boş bir dict oluşturalım\n",
    "    freqs = {}\n",
    "    # (word,1) : 25 -> pozitif cümlelerde geçiyor \n",
    "    # (word,0) : 3 -> negatif cümlelerde geçiyor\n",
    "    for y,tweet in zip(yslist,tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair]+=1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff379e-6682-4f69-9a35-e6bfe9e46098",
   "metadata": {},
   "source": [
    "Yukarıdaki freqs sözlüğü içerisinde her key 2 elamanlı bir tuple içeriyor. (word,y) word preprocess işlemi yapılmış kelime ve y bu kelimenin ait olduğu cümlenin sentimenti 1 veya 0 değeri alıyor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7ab9143-a664-4351-a83e-b14b3d0a9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(freq) : 13171\n"
     ]
    }
   ],
   "source": [
    "freqs = build_freqs(tweets,labels)\n",
    "print(\"len(freq) : \"+ str(len(freqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a22326b-3c0f-4798-8603-98a00330092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('followfriday', 1.0), 25), (('top', 1.0), 32), (('engag', 1.0), 7), (('member', 1.0), 16), (('commun', 1.0), 33), (('week', 1.0), 83), ((':)', 1.0), 3691), (('hey', 1.0), 77), (('jame', 1.0), 7), (('odd', 1.0), 2)]\n"
     ]
    }
   ],
   "source": [
    "print(list(freqs.items())[:10]) #bir kısmını gösterelim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe5d1c-03cf-46dc-a10b-8be5f8d1cef9",
   "metadata": {},
   "source": [
    "# Kelimeleri inceleyelim \n",
    "Yukarıdaki gösterim pek anlaşılır görünmüyor. Bunun için aşağıdaki gibi her kelimenin pozitif ve negatif cümlelerde ne kadar geçtiğini hesaplayalım. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32e613f8-df85-4e60-a1e0-d9b1e245c203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['happi', 212, 25],\n",
       " ['merri', 1, 0],\n",
       " ['nice', 99, 19],\n",
       " ['good', 238, 101],\n",
       " ['bad', 18, 73],\n",
       " ['sad', 5, 123],\n",
       " ['mad', 4, 11],\n",
       " ['best', 65, 22],\n",
       " ['pretti', 20, 15],\n",
       " ['❤', 29, 21],\n",
       " [':)', 3691, 2],\n",
       " [':(', 1, 4584],\n",
       " ['😒', 2, 3],\n",
       " ['😬', 0, 2],\n",
       " ['😄', 5, 1],\n",
       " ['😍', 5, 1],\n",
       " ['♛', 0, 210],\n",
       " ['song', 22, 27],\n",
       " ['idea', 27, 10],\n",
       " ['power', 7, 6],\n",
       " ['play', 46, 48],\n",
       " ['magnific', 2, 0]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# belli başlı kelimeleri seçelim \n",
    "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "data = []\n",
    "for word in keys: \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    if (word,1) in freqs:\n",
    "        pos = freqs[(word,1)]\n",
    "        \n",
    "    if (word,0) in freqs:\n",
    "        neg = freqs[(word,0)]\n",
    "    \n",
    "    data.append([word,pos,neg])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade97c4-cacd-461f-abf5-de07027d297b",
   "metadata": {},
   "source": [
    "Yukarıda gördüğümüz gibi happi kelimesi pozitif cümlelerde 212 kez negatif cümlelerde 25 kez geçiyor. Bu happi kelimesinin pozitif bir anlamı olduğunu gösteriyor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c165514-f9ea-4bdf-a1a4-79acc91cfbc9",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "Yukarıdaki gibi preprocess işlemlerimizi tamamladık bu adımdan sonra sentiment analizi için logistic regression modeli oluşturacağız.  \n",
    "This is a great movie. cümlesi preprocessing işleminden sonra -> [great, movie] ye dönüşür.  \n",
    "Frekans sözlüğü kullanarak yandaki gibi vektör elde ederiz.   \n",
    "xi = [1,3476,245] (bias,pos freq,neg freq) theta = [0.00003,000150,-0.000120]\n",
    "h(xi,theta) = 4.92 değeri alır ve buda Logistic Regression gösteriminde pozitif sentiment bölgesine düşer. \n",
    "\n",
    "#### Eğitim\n",
    "Eğitime başlamadan önce elimizdeki veriyi train/test olarak ayırmamız gerekiyor.\n",
    "Eğitimde cost fonksiyonunu minimize eden theta değerlerini bulmamız gerekiyor. Theta1 ve theta 2 değerlerini random olarak başlatırız ve cost fonksiyonunu minimize ederken theta değerleri için en optimum sonuçları elde ederiz.  \n",
    "\n",
    "Theta değerlerini elde ettikten sonra sıra modelin değerlendirilmesi kısmındadır. Bunun için theta değerleri ile bir değer elde ederiz ve bunu sigmoid fonksiyonuna göndeririz ve tahminimizi alırız.   \n",
    "\n",
    "Bu örnekte sci-kit learn kütüphanesini kullanıyoruz. Sklearn içerisinde bulunan logistic regression sınıfı ile modeli .fit ile eğitiyoruz. \n",
    "\n",
    "### Feature vektörleri oluşturalım\n",
    "Eğitime başlamadan önce her cümle için (1,3) boyutunda özellik vektörleri oluşturalım. Bunu için aşağıda extract_features() metodunu kullanacağız. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30d77cc3-69ed-457b-9456-bd579f98a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = all_positive_tweet[4000:]\n",
    "train_pos = all_positive_tweet[:4000]\n",
    "test_neg = all_negative_tweet[4000:]\n",
    "train_neg = all_negative_tweet[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d35f2c96-2007-4765-a9b0-d944716b599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa075686-f774-44b0-a942-e014fc7319bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(freqs) = 11428\n"
     ]
    }
   ],
   "source": [
    "# tekrardan sadece train seti için freq oluşturuyoruz. \n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "734c2ca3-53a0-44c2-bdfd-bd6dda1fdabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet,freqs):\n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    \n",
    "    # yukarıda bahsettiğimiz gibi [bias,pos,neg]\n",
    "    x = np.zeros((1,3))\n",
    "    \n",
    "    # bias için 1 değeri verelim \n",
    "    x[0,0] = 1\n",
    "    \n",
    "    for word in processed_tweet:\n",
    "        # pos ve neg için artırma işlemi yap \n",
    "        x[0,1] += freqs.get((word,1.0),0)\n",
    "        \n",
    "        x[0,2] += freqs.get((word,0.0),0)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3223dec-ac87-4595-9858-058fbda8ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.133e+03 6.100e+01]]\n"
     ]
    }
   ],
   "source": [
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1088dd5d-d6b8-4d03-b9f9-f803fd7e43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78ae38-9d80-4643-9063-ccf426e95235",
   "metadata": {},
   "source": [
    "Eğitim için preprocessing işlemlerinden geçmiş özellik vektörleri çıkartılmış verileri gönderip gerçek labelları Y ile gönderip modeli eğittik. Eğitim için .fit metodunu kullandık. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec2a3d16-13b1-4175-b639-f4687ec8707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bagat\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add79e2-2411-457f-9a1d-a9d37df33261",
   "metadata": {},
   "source": [
    "#### Modeli Test edelim \n",
    "Modeli test etmek için Xtest , Ytest , theta elimizde bulunan değerleri sigmoid fonksiyonuna göndeririz. Sigmoid bir tahminde bulunur bu tahmin edilen değer 0.5 ten büyükse pozitif, küçükse negatif değeri alır.   \n",
    "Tahmin ettiğimiz değerleri gerçek değerler ile karşılaştırıp modelin başarısını ölçebiliriz.  \n",
    "Aşağıda sci-kit learn kütüphaneasi kullanarak X_test değerleri gönderip clf.predict ile predictionlarımızı yaptık daha sonra accuracy_score metodu kullanarak modelin başarısını ölçtük. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9076aa8-cf61-44db-b4d3-b5dac6b832b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(freqs) = 4607\n"
     ]
    }
   ],
   "source": [
    "# tekrardan sadece train seti için freq oluşturuyoruz. \n",
    "test_freqs = build_freqs(test_x,test_y)\n",
    "\n",
    "print(\"len(freqs) = \" + str(len(test_freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ea94eea1-a6fb-484a-b917-13694fc9ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(test_x), 3))\n",
    "for i in range(len(test_x)):\n",
    "    X_test[i, :]= extract_features(test_x[i], test_freqs)\n",
    "Y_test = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d18420f5-ecd6-46cf-83c5-c0af32ed53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3bcdb2f9-e360-499a-a5f2-8e91d42e5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b621d-a18a-45af-a922-7ffdfea94c66",
   "metadata": {},
   "source": [
    "Modeli test verisinde %99 gibi yüksek bir score elde ediyor. Bu score daha önce görmediği bir veriye göre bir hayli yüksek bir skor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1159a875-993e-4f32-adff-aa20f209d473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ce42459e-6037-45ce-9e66-a6c423cd7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"I love using Twitter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5efbfc01-841a-4107-910e-36809f77ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "test_text_features = extract_features(test_text,freqs)\n",
    "y_pred = clf.predict(test_text_features)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
